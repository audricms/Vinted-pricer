{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5292ae4b-af4b-4fd2-8925-78f83c1e3da1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-25 17:24:20--  https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb\n",
      "Resolving dl.google.com (dl.google.com)... 64.233.167.136, 64.233.167.93, 64.233.167.190, ...\n",
      "Connecting to dl.google.com (dl.google.com)|64.233.167.136|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 104953176 (100M) [application/x-debian-package]\n",
      "Saving to: ‘/tmp/chrome.deb’\n",
      "\n",
      "/tmp/chrome.deb     100%[===================>] 100.09M  37.1MB/s    in 2.7s    \n",
      "\n",
      "2023-12-25 17:24:23 (37.1 MB/s) - ‘/tmp/chrome.deb’ saved [104953176/104953176]\n",
      "\n",
      "Hit:1 https://dl.google.com/linux/chrome/deb stable InRelease\n",
      "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
      "Hit:3 http://archive.ubuntu.com/ubuntu jammy InRelease  \n",
      "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease    \n",
      "Hit:6 https://apt.postgresql.org/pub/repos/apt jammy-pgdg InRelease\n",
      "Fetched 229 kB in 2s (98.9 kB/s)\n",
      "Reading package lists... Done\n",
      "W: https://apt.postgresql.org/pub/repos/apt/dists/jammy-pgdg/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "Note, selecting 'google-chrome-stable' instead of '/tmp/chrome.deb'\n",
      "google-chrome-stable is already the newest version (120.0.6099.129-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 2 not upgraded.\n",
      "Requirement already satisfied: chromedriver-autoinstaller in /opt/mamba/lib/python3.10/site-packages (0.6.3)\n",
      "Requirement already satisfied: selenium in /opt/mamba/lib/python3.10/site-packages (4.16.0)\n",
      "Requirement already satisfied: packaging>=23.1 in /opt/mamba/lib/python3.10/site-packages (from chromedriver-autoinstaller) (23.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/mamba/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/mamba/lib/python3.10/site-packages (from selenium) (0.23.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/mamba/lib/python3.10/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/mamba/lib/python3.10/site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/mamba/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/mamba/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/mamba/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/opt/mamba/lib/python3.10/site-packages/chromedriver_autoinstaller/120/chromedriver'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!wget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb -O /tmp/chrome.deb\n",
    "!sudo apt-get update\n",
    "!sudo -E apt-get install -y /tmp/chrome.deb\n",
    "!pip install chromedriver-autoinstaller selenium\n",
    "\n",
    "import chromedriver_autoinstaller\n",
    "chromedriver_autoinstaller.install()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fe2b8f25-00d7-49ce-9d00-bff651ec1cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: webdriver_manager in /opt/mamba/lib/python3.10/site-packages (4.0.1)\n",
      "Requirement already satisfied: requests in /opt/mamba/lib/python3.10/site-packages (from webdriver_manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/mamba/lib/python3.10/site-packages (from webdriver_manager) (1.0.0)\n",
      "Requirement already satisfied: packaging in /opt/mamba/lib/python3.10/site-packages (from webdriver_manager) (23.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/mamba/lib/python3.10/site-packages (from requests->webdriver_manager) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/mamba/lib/python3.10/site-packages (from requests->webdriver_manager) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/mamba/lib/python3.10/site-packages (from requests->webdriver_manager) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/mamba/lib/python3.10/site-packages (from requests->webdriver_manager) (2023.11.17)\n"
     ]
    }
   ],
   "source": [
    "!pip install webdriver_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2cfc1b4-a594-4f9e-9ded-4d8c15b8faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /opt/mamba/lib/python3.10/site-packages (4.16.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/mamba/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.26.18)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/mamba/lib/python3.10/site-packages (from selenium) (0.23.2)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/mamba/lib/python3.10/site-packages (from selenium) (0.11.1)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/mamba/lib/python3.10/site-packages (from selenium) (2023.11.17)\n",
      "Requirement already satisfied: attrs>=20.1.0 in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (23.1.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/mamba/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/mamba/lib/python3.10/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/mamba/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/mamba/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c816c05a-256a-45d1-8765-918d88806ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas\n",
    "\n",
    "import selenium\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "path_to_web_driver = ChromeDriverManager().install()\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, WebDriverException\n",
    "\n",
    "!pip install -q lxml\n",
    "\n",
    "import bs4\n",
    "import lxml\n",
    "import pandas\n",
    "import urllib\n",
    "\n",
    "from urllib import request\n",
    "from urllib.error import HTTPError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "400bf72f-c730-4b4d-8cfc-4f014832d034",
   "metadata": {},
   "outputs": [],
   "source": [
    "chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "# Adding argument to disable the AutomationControlled flag \n",
    "chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\") \n",
    " \n",
    "# Exclude the collection of enable-automation switches \n",
    "chrome_options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"]) \n",
    " \n",
    "# Turn-off userAutomationExtension \n",
    "chrome_options.add_experimental_option(\"useAutomationExtension\", False) \n",
    " \n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "chrome_options.add_argument(\"--user-agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0'\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f09112c6-7320-4003-974e-e118940e9f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(executable_path=path_to_web_driver)\n",
    "\n",
    "browser = webdriver.Chrome(service=service,\n",
    "                           options=chrome_options)\n",
    "\n",
    "# Changing the property of the navigator value for webdriver to undefined \n",
    "browser.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "31dfccae-fb41-43bf-aa26-eef62277fb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tableau(x) :\n",
    "\n",
    "    browser.get(x)\n",
    "    time.sleep(3)\n",
    "    try:  # handling cookies pop-upabs\n",
    "        Cookies = browser.find_element(By.ID, 'onetrust-accept-btn-handler')\n",
    "        Cookies.click()\n",
    "        time.sleep(2)  # let's be respectful !\n",
    "    except NoSuchElementException:\n",
    "        pass\n",
    "    \n",
    "    scrapping = {}\n",
    "    \n",
    "    for i in range(1,100) :\n",
    "        try :\n",
    "            XPATH = '/html/body/main/div/section/div/div[2]/section/div/div[2]/div[15]/div/div['+str(i)+']'\n",
    "            x = browser.find_element(By.XPATH, XPATH)\n",
    "            classx = x.get_attribute('class')\n",
    "            if len(classx) > 15 :\n",
    "                continue\n",
    "        except NoSuchElementException:\n",
    "            continue\n",
    "        try : \n",
    "            XPATH = '/html/body/main/div/section/div/div[2]/section/div/div[2]/div[15]/div/div['+str(i)+']/div/div/div/div[2]/a'\n",
    "            x = browser.find_element(By.XPATH, XPATH)\n",
    "            scrapping[i] = x.get_attribute('href')\n",
    "            scrapping2={}\n",
    "        except NoSuchElementException:\n",
    "            continue\n",
    "\n",
    "    for i in scrapping.keys() :\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            request_text = request.urlopen(scrapping[i]).read()\n",
    "            page = bs4.BeautifulSoup(request_text, \"lxml\")\n",
    "            scrapping2[i]={}\n",
    "    \n",
    "            prix = page.find('h1')\n",
    "            if prix != None :\n",
    "                scrapping2[i]['Prix (€)'] = prix.text[:-2]\n",
    "            else : \n",
    "                scrapping2[i]['Prix (€)'] = float('nan')\n",
    "        \n",
    "            marque = page.find('a', class_ = 'inverse u-disable-underline-without-hover')\n",
    "            if marque != None :\n",
    "                scrapping2[i]['Marque'] = marque.text\n",
    "            else : \n",
    "                scrapping2[i]['Marque'] = float('nan')\n",
    "        \n",
    "            taille = page.find(size_selector)\n",
    "            if taille != None :\n",
    "                end = taille.text.find('Size information')\n",
    "                scrapping2[i]['Taille'] = taille.text[:end]\n",
    "            else : \n",
    "                scrapping2[i]['Taille'] = float('nan')\n",
    "    \n",
    "            etat = page.find(condition_selector)\n",
    "            if etat != None :\n",
    "                end = etat.text.find('Condition information')\n",
    "                scrapping2[i]['Etat'] = etat.text[:end]\n",
    "            else : \n",
    "                scrapping2[i]['Etat'] = float('nan')\n",
    "        \n",
    "            matiere = page.find(material_selector)\n",
    "            if matiere != None :\n",
    "                scrapping2[i]['Matière'] = matiere.text[:end]\n",
    "            else : \n",
    "                scrapping2[i]['Matière'] = float('nan')\n",
    "        \n",
    "            localisation = page.find(loc_selector)\n",
    "            if localisation != None :\n",
    "                scrapping2[i]['Localisation'] = localisation.text[12:]\n",
    "            else : \n",
    "                scrapping2[i]['Localisation'] = float('nan')\n",
    "        \n",
    "            paiement = page.find(paymeth_selector)\n",
    "            if paiement != None :\n",
    "                scrapping2[i]['Option de paiement'] = paiement.text[19:]\n",
    "            else : \n",
    "                scrapping2[i]['Option de paiement'] = float('nan')\n",
    "        \n",
    "            vues = page.find(nbrview_selector)\n",
    "            if vues != None :\n",
    "                scrapping2[i]['Vues'] = vues.text[14:]\n",
    "            else :\n",
    "                scrapping2[i]['Vues'] = float('nan')\n",
    "    \n",
    "            date_ajout = page.find(added_selector)\n",
    "            if date_ajout != None :\n",
    "                scrapping2[i][\"Date d'ajout\"] = date_ajout.attrs['title']\n",
    "            else : \n",
    "                scrapping2[i][\"Date d'ajout\"] = float('nan')\n",
    "        except HTTPError :\n",
    "            pass\n",
    "            \n",
    "    firstkey = list(scrapping2.keys())[0]\n",
    "    df = pandas.DataFrame.from_dict(scrapping2, orient = 'index', columns = scrapping2[firstkey].keys())\n",
    "    df = df.transpose(copy=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eee08a57-3ead-4a88-a1a4-04c4a4d5b5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_to_url(x) :\n",
    "    wordlist = x.split()\n",
    "    x = wordlist[0]\n",
    "    if len(wordlist)>1 :\n",
    "        for i in wordlist[1:] :\n",
    "            x = x+'+'+str(i)\n",
    "        return x\n",
    "    else :\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d5e77d34-fb5a-46c4-8b6a-fb540cd4761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_selector(tag):\n",
    "\treturn tag.name == \"div\" and tag.has_attr(\"itemprop\") and 'size' in tag.get(\"itemprop\")\n",
    "def condition_selector(tag):\n",
    "\treturn tag.name == \"div\" and tag.has_attr(\"itemprop\") and 'condition' in tag.get(\"itemprop\")\n",
    "def material_selector(tag):\n",
    "\treturn tag.name == \"div\" and tag.has_attr(\"itemprop\") and 'material' in tag.get(\"itemprop\")\n",
    "def loc_selector(tag):\n",
    "\treturn tag.name == \"div\" and tag.has_attr(\"data-testid\") and 'item-details-location' in tag.get(\"data-testid\")\n",
    "def paymeth_selector(tag):\n",
    "\treturn tag.name == \"div\" and tag.has_attr(\"data-testid\") and 'item-details-payment_methods' in tag.get(\"data-testid\")\n",
    "def nbrview_selector(tag):\n",
    "\treturn tag.name == \"div\" and tag.has_attr(\"data-testid\") and 'item-details-view_count' in tag.get(\"data-testid\")\n",
    "def added_selector(tag):\n",
    "\treturn tag.name == \"span\" and tag.has_attr(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "cb14b67a-bb8d-4a21-bce0-f47a0bf59406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_creator(x, plow, phigh, pagenbr) :\n",
    "    x = raw_to_url(x)\n",
    "\n",
    "    return('https://w'+'ww.vinted.fr/catalog?search_text='+x+'&price_from='+str(plow)+'&currency=EUR&price_to='+str(phigh)+'page='+str(pagenbr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "4c2ae2a6-4098-471b-8a73-0dc2181cd591",
   "metadata": {},
   "outputs": [],
   "source": [
    "href1 = h_creator('jean homme', 5, 200, 1)\n",
    "href2 = h_creator('jean homme', 5, 200, 2)\n",
    "href3 = h_creator('jean homme', 5, 200, 3)\n",
    "href4 = h_creator('jean homme', 5, 200, 4)\n",
    "href5 = h_creator('jean homme', 5, 200, 5)\n",
    "href6 = h_creator('jean homme', 5, 200, 6)\n",
    "href7 = h_creator('jean homme', 5, 200, 7)\n",
    "href8 = h_creator('jean homme', 5, 200, 8)\n",
    "href9 = h_creator('jean homme', 5, 200, 9)\n",
    "href10 = h_creator('jean homme', 5, 200, 10)\n",
    "href11 = h_creator('jean homme', 5, 200, 11)\n",
    "href12 = h_creator('jean homme', 5, 200, 12)\n",
    "href13 = h_creator('jean homme', 5, 200, 13)\n",
    "href14 = h_creator('jean homme', 5, 200, 14)\n",
    "href15 = h_creator('jean homme', 5, 200, 15)\n",
    "href16 = h_creator('jean homme', 5, 200, 16)\n",
    "href17 = h_creator('jean homme', 5, 200, 17)\n",
    "href18 = h_creator('jean homme', 5, 200, 18)\n",
    "href19 = h_creator('jean homme', 5, 200, 19)\n",
    "href20 = h_creator('jean homme', 5, 200, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8ebe1f7e-d5bd-4d16-8484-73afe13fb13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrapping des annonces Vinted.fr pour des jeans hommes\n",
    "dfjean1 = tableau(href1)\n",
    "dfjean2 = tableau(href2)\n",
    "dfjean3 = tableau(href3)\n",
    "dfjean4 = tableau(href4)\n",
    "dfjean5 = tableau(href5)\n",
    "dfjean5 = tableau(href5)\n",
    "dfjean6 = tableau(href6)\n",
    "dfjean7 = tableau(href7)\n",
    "dfjean8 = tableau(href8)\n",
    "dfjean8 = tableau(href8)\n",
    "dfjean9 = tableau(href9)\n",
    "dfjean10 = tableau(href10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "7a5035b6-fd6e-453f-91a6-cf40bb91fd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfjean11 = tableau(href11)\n",
    "dfjean12 = tableau(href12)\n",
    "dfjean13 = tableau(href13)\n",
    "dfjean14 = tableau(href14)\n",
    "dfjean15 = tableau(href15)\n",
    "dfjean16 = tableau(href16)\n",
    "dfjean17 = tableau(href17)\n",
    "dfjean18 = tableau(href18)\n",
    "dfjean19 = tableau(href19)\n",
    "dfjean20 = tableau(href20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "f04539f2-1bf1-49cf-9c1e-598537300a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfjean = pandas.concat([dfjean1, dfjean2, dfjean3, dfjean4, dfjean5, dfjean6, dfjean7, dfjean8, dfjean9, dfjean10, dfjean11, dfjean12, dfjean13, dfjean14, dfjean15, dfjean16, dfjean17, dfjean18, dfjean19, dfjean20], axis = 1, ignore_index = True).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "6ebdcd0c-4edc-4678-84fc-15675e10ee5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'je ne sais pas', 'Zara', 'Bonobo', 'Leeyo Jeans',\n",
       "       'Casual by Gémo', 'Redskins', 'Pull & Bear', 'Openfield', 'RAW',\n",
       "       'John H', 'Dockers', 'Brave Soul', 'Devred', 'Autograph', 'ICON',\n",
       "       'Jack & Jones', 'Kiliwatch', 'G-Star', 'sans marque', 'Dsquared2',\n",
       "       'Quiksilver', 'ASOS Design', 'Denim Co', 'Urban Rags',\n",
       "       'Tally Weijl', 'Celio', 'Sixth Sense', 'Bershka', 'Tailor & Son',\n",
       "       'Straight', 'Kiabi', 'Creeks', 'G-Star RAW', 'Siksilk',\n",
       "       'Red Bridge', 'Le Temps des Cerises', 'Ritchie', 'Wrangler',\n",
       "       'Jules', 'SMOG', 'districenter', 'Teddy Smith', 'GovdenimM5888',\n",
       "       'NEW Stone', \"Levi's\", 'Kaporal', 'Uniqlo', 'Sixth June', 'Diesel',\n",
       "       'Lee Cooper', 'Nike', 'Bonobo Jeans', 'Chevignon', 'Armand Thiery',\n",
       "       'Shein', 'Ross Carra', 'H&M', 'Calvin Klein', 'Antony Morato',\n",
       "       'Firetrap', 'BZB', 'Livergy', 'Levi Strauss & Co.', 'GANT',\n",
       "       'Cargo', 'Marithé + François Girbaud', 'RG512', 'Hollister',\n",
       "       'la boutique officielle', 'Lee', 'Salsa', 'Pionier', 'Pepe Jeans',\n",
       "       'Jo Cassidy', 'Benson & Cherry', 'Freeman T. Porter',\n",
       "       'Jeanne Arthes', 'Springfield', 'TERANCE KOLE', 'Grain de Malice',\n",
       "       \"L'Homme Moderne\", 'Regular', 'Frilivin', 'Donovan', 'Crossby',\n",
       "       'Alixpress', 'GAS', 'Tommy Jeans', 'Bleu Marine', 'Bizzbee', 'C&A',\n",
       "       'The Rockn Rev', 'Lewis', 'Gémo', 'AARHON', 'Beau Temps Belle Mer',\n",
       "       'Babolat'], dtype=object)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfjean['Marque'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "25126cd3-32d6-4e6a-a99e-dd2555eaf97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nettoyage des données\n",
    "for i in dfjean['Marque'].where(dfjean['Marque']=='je ne sais pas').dropna().index :\n",
    "    dfjean['Marque'][i] = float('nan')\n",
    "for i in dfjean['Marque'].where(dfjean['Marque']=='sans marque').dropna().index :\n",
    "    dfjean['Marque'][i] = float('nan')\n",
    "for i in dfjean['Marque'].where(dfjean['Marque']=='Levi Strauss & Co.').dropna().index :\n",
    "    dfjean['Marque'][i] = \"Levi's\"\n",
    "for i in dfjean['Marque'].where(dfjean['Marque']=='G-Star RAW').dropna().index :\n",
    "    dfjean['Marque'][i] = 'G-star'\n",
    "for i in dfjean['Marque'].where(dfjean['Marque']=='G-star').dropna().index :\n",
    "    dfjean['Marque'][i] = 'G-Star'\n",
    "for i in dfjean['Marque'].where(dfjean['Marque']=='Bonobo Jeans').dropna().index :\n",
    "    dfjean['Marque'][i] = 'Bonobo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "9b86f679-4ef1-4984-b98e-34a1799a573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfjean.to_csv('jeanshomme.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fc775a-021f-4fcb-afec-95773208160b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
